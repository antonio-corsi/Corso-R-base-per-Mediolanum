
# statistica descrittiva (analisi esplorativa dei dati) --------------------------------------------------

# la parte del corso pi√π importante: prima di PREVEDERE occorre CAPIRE i propri dati.
# grande quantit√† di statistiche disponibili in R (base o contributed).

library(ISLR)
X <- Credit$Income   # variabile numerica continua (reale)

# misure di CENTRATURA: mean, median

mean(X)                    # --> 45.22 (sensibile agli outlier)
sum(X)/400                 # stesso risultato

vet.es1 <- c(1,2,3,4,5)
vet.es2 <- c(1,2,3,4,100)
mean(vet.es1)
mean(vet.es2)

median(vet.es1)
median(vet.es2)
vet.es3 <- c(1,2,3,4)
median(vet.es3)            # 2 posizioni centrali uguali ("tie": legami) --> la loro media

median(X)                  # pi√π "robusta" (√® una metrica posizionale)
X

# misure di variabilit√†
sd(X)                      # la deviazione standard --> 35.24
sd(vet.es1)
sd(vet.es2)                # perch√® influenzata dall'outlier 100

var(X)
sd(X)^2
sqrt(var(X))               # la varianza √® semplicemente il quadrato della sd, ovvero, la sd √® la radice quadrata POSITIVA
                           # della varianza.
                           # la varianza NON √® espressa nella unit√† di misura dei dati, e ci√≤ √® spesso un problema.

mad(X)                     # median absolute deviation (pi√π robusta agli outlier).
mad(vet.es1)
mad(vet.es2)
                           # bottom-line: 'mad' √® l'equivalente per le metriche di variabilit√† della 'mediana' nelle metriche
                           # di centratura

# esempio di radici:
sqrt(9)                    # --> fornisce la "principal square root" (vedi help R), cio√® la sola radice quadrata positiva.

sd(X)/mean(X)              # CV (Coefficiente di variazione) --> tra 0 e 1 --> qui molto alto.
                           # in generale, √® possibile calcolare il CV perch√® la 'sd' ha la stessa unit√† di misura dei dati!
x11()
hist(X,breaks = 20)        # alta variabilit√† (qui rilevata tramite un istogramma, cio√® un grafico delle frequenze).
                           # 'breaks' √® il numero di BIN, R di default ne sceglie uno adatto al dataset.
                           # Regole pratiche di scelta numero 'breaks' ottimale:
                           # - la radice quadrata di n (il numero di osservazioni, cio√® di righe) --> maggiore del default;
                           #   sqrt(length(X))=20 (il valore usato qui da me, infatti);
                           #   vedi ad esempio: "https://www.statsandr.com/blog/outliers-detection-in-r/"
                           # - prof.ssa Vicario: 1 + (10/3) * log10(n), dove log10 √® il logaritmo in base 10 [in R usare la
                           #   funzione 'log10()'] ed n √® il numero di osservazioni (Vicario, p. 105); QUELLO di 'hist'?!
                           # - prof. Brandimarte: min. 5 - max. 20, mai classi vuote;

                           # l'argomento 'freq' di 'hist' √® importante:
                           # TRUE (il default) visualizza in ordinata le frequenze ASSOLUTE (il loro conteggio);
                           # FALSE visualizza in ordinata le frequenze RELATIVE, ovvero la base per stimare le probabilit√†
                           # (in modo frequentista), in modo tale che l'intera area dell'istogramma ha somma 1.


bin.vicario <- round(1 + (10/3 * log10(length(X))),0);bin.vicario # [la funzione 'round(X,0)' arrotonda all'intero pi√π vicino]
hist(X,bin.vicario)        # lo stesso numero di 'hist'?!

hist(X,freq=F,breaks = 20) # osservate come l'asse delle X riporti solo alcuni valori di riferimento (50,100,150) --> R infatti √®
                           # minimalista, mostra sempre il minimo delle informazioni, le altre, un'infinit√† volendo, si ottengono specificando
                           # esplicitamente alcuni argomenti. Nel prossimo corso vedremo diversi altri esempi di questo minimalismo di R.

# 2 tipi di istogramma, come detto prima:
hist(X)                    # in ordinata ci sono le frequenze assolute
hist(X,freq=F)             # in ordinata ci sono le frequenze relative (cio√® le frequenze assolute divise per la dimensione
                           # del campione, qui 'length(X)'), che costituiscono una stima (frequentista) delle probabilit√†.

# sovrapposizione all'istogramma di una curva "LISCIA" (smooth): √® una "stima kernel" della PDF, non-parametrica (RIA, 2nd ed, p. 127).
lines(density(X),col="red",lwd=3) # le densit√† sono la stima della probabilit√† di X (Income) calcolata in base alle frequenze RELATIVE.
plot(density(X))                  # le densit√† sono calcolabili a prescindere dall'istogramma (discreto, strutturato bins) e quindi plottabili
                                  # indipendentemente.
                                  # per√≤, se voglio plottare le densit√† SOPRA l'istogramma, esso deve essere stato creato con freq=F.


# --> sezione finale su "variabilit√†" (dimensione del campione)

# funzione di sintesi:
sum(X)
min(X)
max(X)
range(X)         # min e max
summary(X)       # gi√† vista

# R base NON fornisce le funzioni: MODA (valore pi√π probabile) e le due misure di SHAPE (skewness e curtosi)
# opzione a): si possono creare (vedi fondo - sezione apposita)
url <- "https://codeburst.io/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa"
browseURL(url, browser = getOption("browser"))

mean(X)
median(X)
Mode(X)  # definita nella sezione 'statistiche mancanti in R base'

url <-  "https://www.excelr.com/skewness-and-kurtosis"
browseURL(url, browser = getOption("browser"))

my.vars <- c("Income","Age","Balance")
round(sapply(Credit[my.vars],mystats),3)   # la funzione 'mystats' √® definita nella sezione 'statistiche mancanti in R base'.
                                           # --> le ultime due righe dell'output riportano skewness e kurtosis:
                                           #     a) le tre distribuzioni (delle tre variabili) sono skewed (assimetriche)
                                           #     verso dx (valori positivi della skewness), in particolare 'Income' e 'Balance'
                                           #     come si pu√≤ vedere dai rispettivi istogrammi. 'A ge' lo √® pochissimo.
                                           #     b) 'Income' ha un picco (curtosi) pi√π alto della gassiana (perch√® valore positivo),
                                           #     le altre due variabili sono un p√≤ pi√π piatte della gaussiana (perch√® valore negativo).

                                           #     Quest'ultima cosa si pu√≤ vedere cos√¨ (per 'Income'):

X<-Credit$Income
hist(X,freq = FALSE,breaks=30)
xfit <- seq(min(X),max(X),length=40)       # la griglia di valutazione
yfit <- dnorm(xfit,mean=mean(X),sd=sd(X))  # le densit√† gaussiane corrispondenti alla griglia (con quei parametri)
lines(density(X),col="blue")               # la linea "smooth" sovraimposta
lines(xfit,yfit,col="red")                 # la gaussiana sovraimposta (pi√π bassa, in effetti)

# opzione b) usare qualche pacchetto:
library(psych)   # un package con molte funzioni utili
describe(X)      # contiene anche skewness e curtosi [vedi libro su R nel Marketing]

# [funzioni matematiche (funzionamento vettoriale)]:
sqrt(X)   # radice quadrata
X^2       # elevamento a potenza
log(X)    # logaritmo
url <- "https://towardsdatascience.com/overview-of-40-mathematical-functions-in-r-4b4e138685ff"
browseURL(url, browser = getOption("browser"))
# ecc --> vedi anche qualche cheatsheets su google]

range(X)
summary(X)
quantile(X)       # come 'summary'
                  # ci sono molti modi di calcolarli (vedi help R), leggermente differenti (vedi argomento 'type')
                  # european quantile: Pr[X <= q]; american quantile: Pr[X > q]

q.40 <- quantile(X,0.4)   # il "quantile", a volte anche detto "percentile" (so 156778), √® UN VALORE SPECIFICO della X (√® un income);
q.40                      # --> cio√®, il 40% delle osservazioni ha valore di Income <= 27.81
                          #     e quindi il restante 60% delle osservazioni ha valore di Income > 27.81

hist(X,breaks = 30)
abline(v=q.40,col="Red",lty=2,lwd=3)   # funziona SOLO sopra un grafico gi√† prodotto

fivenum(X)        # "returns Tukey's five number summary (minimum, lower-hinge, median, upper-hinge, maximum)"

plot(ecdf(X))     # la CDF empirica --> conferma graficamente che un valore di Income intorno a 27.81 lascia alla sua sx il 40% delle obs.
                  # In generale, la CDF (Cumulative Distribution Function) riporta in ordinata la Prob(X<=X.k), dove X.k √® un valore dato
                  # in ascissa.

# funzioni grafiche di una variabile numerica:
par(mfrow=c(1,2)) # divide la finestra in due
boxplot(X,outline = T)        # --> la distribuzione campionaria, con varie evidenze.

url <- "https://quantgirl.blog/anatomy-of-a-box-plot/"
browseURL(url, browser = getOption("browser"))   # l'utilit√† del boxplot e come interpretarlo

# uno dei vari scopi del boxplot √® di vedere gli outlier;
# cos'√® un outlier? varie definizioni: in statistica descrittiva √® un'osservazione ai limiti del range di valori;
# ad esempio:
c(quantile(X,0.25)-1.5*IQR(X),quantile(X,0.75)+1.5*IQR(X)) # un outlier √® oltre Q1-1.5*IQR e Q3+1.5*IQR
c(mean(X)-3*sd(X),mean(X)+3*sd(X))                         # un outlier √® oltre -3sigma e + 3sigma (pi√π conservativo)

hist(X)           # distribuzione assimmetrica (non gaussiana)
par(mfrow=c(1,1)) # restore del layout grafico UNICO
# --> presenza di outlier (confermati) e non-gaussianit√† IMPEDISCONO l'applicazione di diversi metodi di Machine Learning.
# [more on outlier √® nel corso R avanzato]

# Income √® gaussiana?
shapiro.test(X)   # test di gaussianit√† --> no, X NON √® gaussiana (perch√® p.value < 0.05)
                  # 2.2 * 10^-16 = 0 del computer a doppia precisione

par(mfrow=c(1,2))
hist(X,breaks = 30)
# come gaussianizzare X?
X.log <- log(X)               # compatta il range e rende la distribuzione pi√π simmetrica
hist(X.log,breaks = 30)       # molto pi√π simmetrica!
shapiro.test(X.log)           # il test √® molto sensibile, rifiuta perch√® X NON √® proprio gaussiana

par(mfrow=c(1,2))
hist(X,main = "istogramma",breaks = 50)     # frequenze (RILEVATE dal campione)
d <- density(X)                             # la forma
plot(d,main="stima kernel della distribuzione di income")  # a sx del min(income) STIMA la forma della funzione
polygon(d,col="red",border="blue") # sovrascrive
rug(X,col="brown")                 # sovrascrive
par(mfrow=c(1,1))

# Nota su standardizzazione:
(X.std <- scale(X,center=T,scale=T))  # standardizazione, cio√®: centratura e rescaling (ovvero diviso per la dev. std);
                                      # i due attributi riportati in coda sono media e varianza originali;
                                      # gli attributi sono anche visibili con 'attributes(object.name)'.

hist(X.std)
mean(X.std)                           # 0 (-2.9142e-18 = -291.42e-16)
sd(X.std)                             # 1 (√® sempre lo scaling DOPO la centratura - vedi help R)

# diverse tecniche di ML richiedono che i dati siano standardizzati:
# - l'inferenza (altro corso)
# - quando si calcolano "distanze" (ad es. KNN, PCA, clustering, FA, ecc)
# - ecc

# uno dei grandi vantaggi della 'standardizzazione' √® di poter CONFRONTARE distribuzioni differenti; esempio:
par(mfrow=c(1,2))
hist(Credit$Income)
hist(Credit$Age)
par(mfrow=c(1,1))
par(mfrow=c(1,2))
hist(scale(Credit$Income))
hist(scale(Credit$Age))
par(mfrow=c(1,1))
# --> vediamo qui chiaramente che la variabilit√† di 'Income' √® di -1sd / +4sd, molto maggiore di quella di 'Age',
# che √® di -2sd / + 2sd.
# Un'informazione SIMILE la posso ottenere con 'range'.


# attenzione: spesso si usa (impropriamente) il termine 'normalizzazione', che invece √® la
# la trasformazione dei dati in gaussiani.

# centratura:
X.std <- scale(X,center=T,scale=F) # solo centratura (cio√® uno shift)
mean(X.std)                        # --> e-16 √® 0 al computer
sd(X.std)                          # quella originale
scale(X,center=T,scale=F)          # riporta in coda solo il centro originale! (la sd non √® cambiata)

# rescaling (caso articolare):
X.resc <- scale(X,center=F,scale=T) # solo rescaling
mean(X.resc)                        # 0.78
sd(X.resc)                          # 0.61

x11()
par(mfrow=c(2,2))
hist(X)
hist(Y)
hist(scale(X,scale=T))              # 'hist' dei dati riscalati (cio√® divisi per la sd)
hist(scale(Y,scale=T))

# "When data is rescaled the median, mean(Œº), and standard deviation(œÉ) are all rescaled by the same constant."
# https://www.statisticshowto.com/what-is-rescaling-data/

# "The root-mean-square for a (possibly centered) column is defined as sqrt(sum(x^2)/(n-1)), where x
# is a vector of the non-missing values and n is the number of non-missing values. In the case center = TRUE,
# this is the same as the standard deviation, but in general it is not. (To scale by the standard deviations
# without centering, use scale(x, center = FALSE, scale = apply(x, 2, sd, na.rm = TRUE)).)

# destandardizzazione (anch'essa vettoriale):
X.destd <- (X.std * sd(X)) + mean(X)
# verifica:
head(X.destd,10)
head(X,10)
# fine NOTA su standardizzazione]


Y <- Credit$Gender   # una variabile categorica
class(Y)
levels(Y)
table(Y)
summary(Y)           # uguale a 'table(Y)'

# statistiche su DUE variabili numeriche:

X.1 <- Credit$Income
X.2 <- Credit$Age

cov(X.1,X.2)         # valore assoluto, non diviso per le sd delle due variabili;

cor(X.1,X.2)         # in genere pi√π utile della covarianza, perch√® confrontabile tra variabili;
                     # per default, il comando 'cor' usa il metodo di Pearson;
                     # la correlazione √® semplicemente la covarianza standardizzata --> un numero tra -1 e 1,
                     # dove 0 --> non-correlazione, +1/-1 --> massima correlazione (l'intensit√† correlazione);
                     # |cor| > 0.5: interessante; |cor| > 0.7: alta (dipende dal dominio: nel "socio-demografico
                     # correlazioni in genere pi√π basse; nei domini scientifici-ingegneristici correlazioni pi√π alte,
                     # mondo finance: via di mezzo);
                     # correlazione positiva (>0) --> le due variabili crescono OPPURE diminuiscono INSIEME;
                     # correlazione negativa (<0) --> al crescere di una variabile, l'altra decresce, o viceversa;
                     # metodi di calcolo della correlazione (argomento 'method'): Pearson (default), Spearman (pi√π "robusta"
                     # perch√® usa le posizioni (ranking) anzich√® i valori, ed in grado di misurare anche le relazioni non-lineari);
                     # la correlazione INDICA ASSOCIAZIONE tra le due variabili, NON CAUSA-EFFETTO!
                     # --> Vedi il libro "How to lie with statistics" di Darrell Huff (1954).
                     # la regressione si fa, poi, quando c'√® causa-effetto! [Vicario].
                     # nella esplorazione dei dati la correlazione si misura su una COPPIA di variabili (numeriche). Nella
                     # regressione, che √® una tecnica di previsione che vedremo nel prossimo corso, si misura anche la
                     # possibile correlazione tra molte variabili, che prende il nome di "multi-collinearit√†".

cor(X.1,X.2,method = "spearman")  # la correlazione calcolata in modo "robusto agli outlier", cio√® con le posizioni anzich√® i valori;
                                  # inoltre, individua anche relazioni non-lineari.
x11()
plot(X.1,X.2,col="black")  # si chiama 'scatterplot'
X.1.jit <- jitter(X.1)     # "add a small amount of noise to a numeric vector." Evita (l'eventuale, non qui) overlap dei punti
X.2.jit <- jitter(X.2)
points(X.1.jit,X.2.jit,col="red") # 'points' stampa SOPRA un plot gi√† aperto (altre funzioni cos√¨, es. 'lines')

# un grafico interattivo (vedi altri in seguito):
x11()                             # eseguire l'intero blocco di istruzioni - X11 toglie il focus all'editor!
                                  # x11 crea un nuove device grafico, dimensionabile con gli argomenti 'width' e 'height'
plot(X.1,X.2);identify(X.1,X.2)   # individuazione punti,
                                  # si esce dal plot con escape.
                                  # si ferma l'identificazione punti con il bottone 'stop'
dev.off()                         # chiude la finestra

?Devices()           # lista dei devici grafici disponibili - ne abbiamo gi√† visti alcuni

var.num <- c("Income","Limit","Rating","Cards","Age","Education","Balance")
cor(Credit[var.num])              # matrice di correlazione (difficile visivamente da controllare!)

library(corrplot)
corrplot(cor(Credit[var.num]),type="upper",method="ellipse")
                                             # la matrice di correlazione √® sempre SIMMETRICA, per definizione,
                                             # cio√® se si scambiano righe e colonne si ottiene la stessa identica matrice;
                                             # dunque √® sufficiente visualizzare la matrice triangolare superiore
                                             # (od inferiore).
                                             # pi√π l'ellisse √® allungata maggiore √® l'intensit√† della correlazione.
# si vede una fortissima correlazione tra 'Limit' e 'Rating' ed una forte tra 'Balance' e 'Limit'
cor(Credit$Limit,Credit$Rating)   # 0.99!
cor(Credit$Limit,Credit$Balance)  # 0.86

# tre errori tipici:
# 1) x11 √® rimasto aperto (cio√® non avete fatto 'dev.off()') --> il nuovo grafico √® visualizzato nel device grafico x11

# 2)
corrplot(Credit)           # la correlazione tra 2 variabili categoriche (o tra una variabile categorica ed una numerica) non ha senso.

# 3)
corrplot(
  Credit[var.num])
# si deve applicare la funzione 'corrplot' alla matrice delle correlazioni, non al dataframe originale!

# un'alternativa al "correlation plot" √® il "correlation gram" (stesso scopo):
library(corrgram)
corrgram(Credit[var.num],order=T,lower.panel=panel.shade,upper.panel=panel.pie,main="Correlation gram di Credit")
                   # la parte bassa si interpreta cos√¨:
                   # - una cella blu con diagonali dal basso all'alto indica correlazione positiva;
                   # - una cella rossa con diagonali dall'alto al basso indica correlazione negativa;
                   # - l'intensit√† del colore (rosso o ble) indica l'intensit√† della correlazione (da 0 a 1, oppure da 0 a -1);
                   #   la parte alta si interpreta cos√¨:
                   # - l'intensit√† della correlazione √® data sia dal colore che dalla dimensione della fetta colorata

# esempio APPLICATIVO (finanziario) dell'utilit√† delle correlazioni 
# (dal libro "Statistic@online" di Bollani e Bottacin, p. 206)

library("openxlsx") # fornisce la funzione 'read.xlsx' (non disponibile in R base)
# per la lettura in R di file excel vedi:
url <- "https://www.r-bloggers.com/2021/06/reading-data-from-excel-files-xlsxlsxcsv-into-r-quick-guide/"
browseURL(url, browser = getOption("browser"))

data <- read.xlsx(xlsxFile = "C:/Users/Utente/Desktop/salvataggi/SALVATAGGIO DATI/Documents/Seminari/Data Science (corsi)/Corso R base (iCubed - Banca di Asti, RMA)/Correlazione.xlsx")
# i dati sui REATI di riciclaggio scoperti e denunciati nel 2016 sono forniti dall'ISTAT.
# i dati degli SPORTELLI nel 2016 sono forniti da Banca d'Italia,
# i dati sul numero di segnalazioni di operazioni sospette (SOS) nel 2016 sono forniti dalla UIF (Unit√† di Informazione
# Finanziaria)
# X = reati / sportello: numero medio per sportello di reati di riciclaggio denunciati nel 2016
# Y = SOS / sportello: numero medio per sportello di SOS nel 2016

dim(data)
str(data)
View(data)

cor(data$X,data$Y) # --> 0.68
                   #     cio√® nel 2016 c'√® stata una correlazione positiva tra reati denunciati (per sportello)
                   #     e SOS (per sportello); ci√≤ conferma che, se la correlazione √® confermata negli anni
                   #     successivi, le SOS sono uno strumento efficace nell'anti-riciclaggio.


# statistiche/funzioni su due variabili categoriche:
X <- Credit$Gender
Y <- Credit$Student
cov(X,Y)             # --> NO!
table(X,Y,useNA = "ifany")           # tabella a doppia entrata (two-way table);
                                     # √® una tabella di contingenza a 4 valori (aka, tetracorica - Wikipedia)
                                     # ['useNA = "ifany"' rimuove eventuali MV]
addmargins(table(X,Y))
addmargins(table(X,Y),FUN="min")    # la colonna margine pu√≤ riportare varie funzioni (vedi help)

plot(X,Y)

# il test chi-quadrato di Pearson
# [MATH. se due eventi sono indipendenti, la loro frequenza CONGIUNTA √® il prodotto delle frequenze MARGINALI]
chisq.test(X,Y)       # H0: indipendenza;
                      # accetto, sono indipendenti (p.val alto per qualsiasi alpha);
                      # non d√† l'intensit√† della eventuale dipendenza, come invece fa la correlazione
                      # (servono altre tecniche, come gli "odds ratio")

# --> sul test chi-quadrato vedi le slide apposite nel corso OEC016

# un'alternativa alla correlazione per le variabili categoriche √® la cross-tabulazione (Jank):
library(gmodels)
CrossTable(X,Y)                      # cell contents in alto:
                                     # N: frequenza congiunta assoluta
                                     # N/Table Total: frequenza congiunta relativa
                                     # "chi-square distribution" √® il contributo di quella cella al valore della statistica chi-square.
                                     # N/Row Total e N/Col Total sono le frequenze condizionate; non confonderle con le
                                     # frequenze marginali (riportate sui totali di colonna e di riga):

url <- "http://progettomatematica.dm.unibo.it/StatisticaDescrittiva/StatisticaDescrittiva/cap5.html"
browseURL(url, browser = getOption("browser"))

library(vcd)
assocstats(table(X,Y))               # l'intensit√† della dipendenza, misurata con 3 differenti "indici di connessione";
                                     # qui indici bassi.
                                     # il termine "indice di connessione" √® dello statistico italiano Corrado Gini.
                                     # la connessione √® concetto ampio, adattabile anche a due variabili numeriche;
                                     # poich√® tuttavia le variabili numeriche possono essere ordinate, si pu√≤
                                     # calcolarne media e deviazioni, ecc, si preferisce calcolarne la correlazione
                                     # (anzich√® appunto la connessione).
                                     # per il calcolo degli indici di connessione (ad es. il chi-quadro) --> vedi prossimo corso

# statistiche  su due variabili (una numerica ed una categorica):
X <- Credit$Income   # variabile numerica continua
Y <- Credit$Student
cov(X,Y)             # --> NO!
cor(X,Y)

plot(X,Y)

# --> boxplot appaiati ed ANOVA (prof. Tardella, La Sapienza, 2-a giornata)

# funzioni su tre variabili categoriche:
X <- Credit$Gender
Y <- Credit$Student
Z <- Credit$Married
table(X,Y,Z)                         # frequenze congiunte


# funzioni sinottiche (agiscono su ALCUNE/TUTTE le colonne)
my.vars <- c("Income","Age","Education","Balance")
colMeans(Credit[my.vars])
x11()
par(mfrow=c(2,2))
boxplot(Credit[my.vars])         # sd troppo diverse tra loro:
apply(Credit[my.vars],2,sd)
boxplot(scale(Credit[my.vars]))  # sd ora confrontabili, perch√® dati riscalati

library(GGally)
ggpairs(Credit[my.vars]) # output molto informativo
library(Hmisc)
describe(Credit[my.vars])

# esame delle relazioni tra 3+ variabili categoriche (RIA, 2nd ed., p. 276-278)
library(vcd)
my.var <- c("Education","Gender","Student")
ftable(Credit[my.var])  # --> l'output √® ordinato secondo il vettore di variabili fornito

mosaic(~Education+Gender+Student,data=Credit,shade=T,legend=T)

# prima di interpretare questo grafico estremamente informativo, ne esaminiamo uno analogo, dal famoso dataset 'Titanic':

str(Titanic)   # --> fornisce il numero di passeggeri sovravvissuti o morti, cross-classificati per Classe (1-a, 2-a, 3-a,
               #     equipaggio), sesso (uomo, donna), et√† (adulto, bambino).

mosaic(Titanic,shade = T)
               # deduzioni dal grafico (si devono considerare le altezze RELATIVE a sx, a dx, in basso ed in alto):
               # - l'equipaggio era il pi√π numeroso, seguito dalla terza classe; la prima classe era la meno numerosa;
               # - l'equipaggio era composto in grande maggioranza da uomini e praticamente tutto da adulti (tranne poce eccezioni);
               #   le classi con pi√π bambini erano la terza e la seconda, quella con pi√π donne la prima;
               # - la maggior quantit√† di deceduti si √® avuta tra l'equipaggio, che si √® evidentemente molto sacrificato, ed a
               #   decrescere in base alla classe: i pi√π ricchi si sono salvati in maggior numero;
               # - donne e bambini si sono salvati molto pi√π degli uomini, como noto dalle cronache, ma comunque di pi√π nelle classi
               #   alte (prima e seconda)
               # --> il colore BLU indica che quella cross-classificazione √® risultata pi√π numerosa di quello che ci si sarebbe
               #   potuto attendere se le variabili fossero indipendenti; il colore ROSSO, all'oppopsto, indica che quella
               #   cross-classificazione √® risultata meno numerosa di quello che ci si sarebbe potuto attendere se le variabili
               #   fossero indipendenti.

example(mosaic)  # vari possibili layout con differenti dataset


# ora, esaminiamo nuovamente il grafico mosaic per i dati categorici di 'Credit':
mosaic(~Education+Gender+Student,data=Credit,shade=T,legend=T)
               # --> l'assenza di colori ci testimonia che le frequenze osservate sono abbastanza vicine a quelle che avremmo
               #     con variabili indipendenti. Confermato dalle altezze relative.

# funzioni di gruppo (RIA, 2nd ed. pp. 142+)
# - aggregate
# - by (simile al comando SQL 'group by')
# --> agiscono sui gruppi di righe aggregate (anzich√® sulle righe singole)

# variabilit√† -------------------------------------------------------------

library(ISLR)

X <- Credit$Income
sd(X)   # con 400 osservazioni!

# NB. la standard deviation √® sempre > 0! (in particolare, non √® MAI negativa).
# La sd PUO' essere uguale a zero SOLO SE il campione estratto √® costante.

# Il termine "standard deviation" √® stato introdotto in statistica da Pearson nel 1894 assieme alla lettera greca sigma
# che lo rappresenta.

# sd calcolata dalla funzione R e, per confronto, manualmente (a passi) con la formula (Wikipedia IT)
sd(X)                         # --> 35.24
scarti <- X-mean(X)           # --> il vettore degli scarti (nella formula: Xi - X-bar)
scarti.quad <- scarti^2       # --> il vettore deglli scarti al quadrato (nella formula l'apice 2) 
ssr <- sum(scarti.quad)       # --> il numeratore della formula (in gergo statistico SSR = Sum of squared Residual) 
(X.var <- ssr/(length(X)-1)) 
(X.std <- sqrt(X.var))        # --> 35.24

# la funzione 'sd' divide per n-1 gradi di libert√†:
sqrt(sum(scale(X,center=T,scale=F)^2)/(length(X)-1)) # formula della deviazione standard (vedi Wikipedia):
# la radice quadrata della somma degli scarti al quadrato divisi per n-1.
# n-1 √® un aggiustamento numerico (perch√® la media della popolazione √® stata stimata con la media campionaria,
# e quindi si √® perso un grado di liberta - i gdl sono le osservazioni).
# attenzione nei piccoli campioni! --> /n oppure /n-1 √® ben diverso!

# 'scale(X,center=T,scale=F)' equivale a 'X-mean(X)'.
# NB. il suriportato comando illustra il funzionamento VETTORIALE di R.

# la standard deviation √® una misura di VARIABILITA' dei dati (come var e mad).
# la variabilit√† si riduce al crescere del numero di osservazioni (le righe del df).
# [pensiamo alle proiezioni delle elezioni: la forchetta di variabilit√† della percentuale ottenuta da un certo partito
# si riduce mano a mano che i seggi sono scrutinati].
# perch√® ci√≤? se la dimensione del campione √® bassa, la presenza di uno o pochi OUTLIER pu√≤ influenzare molto la media
# (la deviazione standard √® infatti una MEDIA, per questo √® anche detta "scarto quadratico medio").

# verifichiamo questo fenomeno:
m <- dim(Credit)[1]
n <- dim(Credit)[2]
m;n                     # m,n √® notazione di algebra lineare.
                        # nel machine learning si usa in genere n,p (ad es. nel licorrelazione ISLR)

sd.set <- double(m)                   # inizializzazione a zero del vettore (pi√π veloce)

for (i in 1:m) {
  # X <- Credit$Income[1:i]                     # dati estratti SEQUENZIALMENTE (i precedenti outlier continuano ad influenzare!)
  # X <- sort(Credit$Income[1:i])               # dati estratti in ORDINE (c'√® comunque un BIAS nei dati, cio√® una deformazione
                                                # introdotta dall'ordinamento)
  X <- sample(Credit$Income,i,replace=FALSE)    # dati estratti CASUALMENTE (meglio, diversi)
  sd.set[i] <- sd(X)
}

plot(sd.set,xlab="Dimensione del campione",ylab="Valore della deviazione standard")
# --> dopo 50 osservazioni circa la deviazione standard si stabilizza (intorno a 35)

# nota sul campionamento (sampling):
set.seed(1)                           # per la riproducibilit√† dei risultati (random seed)
sample(1:10,3,replace=FALSE)          # non c'√® MAI due volte lo stesso elemento (dentro la tripletta estratta)!
                                      # verificarlo selezionando il comando e ripetendo 'Run' diverse volte.
sample(1:10,3,replace=TRUE)           # lo steso elemento pu√≤ cos√¨ invece ricomparire nella tripletta estratta.

# debug del ciclo (utile ad esempio per vedere variabili LOCALI ad un ciclo):
# - breakpoint e poi 'Source' (e non 'Run')
# - la prima sd √® 'NA' perch√® non c'√® variabilit√† con una sola osservazione
sd(Credit$Income[1:1])   # --> NA
sd(Credit$Income[1:2])   # --> ok

# SE LA VARIABILITA' DEL FENOMENO/CAMPIONE SOTTO STUDIO E' BASSA, LE PREVISIONI SONO PIU' AFFIDABILI.
# qual √® dunque la dimensione minima (ottimale) del campione? campionare COSTA!
# se la variabile √® solo una, servono almeno 30-50 osservazioni (la causa √® "il teorema del limite centrale");
# se le variabili sono parecchie, come spesso accade, servono almeno:
# - 10 osservazioni per ogni variabile  # p=3 --> versione lasca: almeno n = 30 oss (per ridurre la sd);
#                                                 versione conservativa: 10 per la prima var, altre 10 per ognuna dei valori
#                                                 della prima var, etc --> 10^3=1000
# - oppure: n > 5(p+2); p=3 --> 5(3+2) = 25 (simile alla prima regola lasca)
# attenzione: sono "regole del pollice"!

# per una trattazione pi√π teorica (matematica) di questo tema (la dimensione del campione), vedi il PDF
# allegato, con tre pagine estratte da un libro del prof. Brandimarte di qualche anno fa. [scusate le note
# mie a mano!]


# funzionamento vettoriale di R -------------------------------------------
# ["Advanced R" di Hadley Wickham - p. 359 e pp. 366+]
# Esempio:
var.num <- c("Income","Limit","Rating","Cards","Age","Education","Balance")

system.time(colSums(Credit[,var.num]))       # funzionamento vettoriale
system.time(apply(Credit[,var.num],2,sum))   # funzionamento non vettoriale (pi√π lento)


# studio sulla variabilit√† ----------------------------------------------------------------------------------------------------------------------------------------------------

# Installazione pacchetti (se non gi√† presenti)
install.packages("ISLR")

library(ISLR)

# Dataset disponibile direttamente
data("Credit")
str(Credit)

# A. Calcolo della correlazione
cor(Credit$Balance, Credit$Income)
cor.test(Credit$Balance, Credit$Income, method = "pearson")

library(ISLR)
data("Credit")

# Correlazione originale tra Balance e Income
cor(Credit$Balance, Credit$Income)

# 1) Raddoppio devstd di Income mantenendo la media (cor non cambia)
mu <- mean(Credit$Income)
sd <- sd(Credit$Income)
Credit$Income2 <- mu + 2 * (Credit$Income - mu)
cor(Credit$Balance, Credit$Income2)  # ‚âà uguale alla precedente

# 2) Aggiungo rumore forte a Income (cor diminuisce)
set.seed(123)
Credit$Income_noisy <- Credit$Income + rnorm(nrow(Credit), 0, 100)
cor(Credit$Balance, Credit$Income_noisy)  # molto pi√π bassa

# Infatti, immagina di misurare reddito e saldo della carta:
# - senza errori ‚Üí c‚Äô√® correlazione positiva (chi guadagna di pi√π tende ad avere pi√π debiti).
# - se a ogni reddito aggiungo un errore enorme (es. ¬±100.000 ‚Ç¨ casuali), i valori diventano confusi:
#   alcune persone con basso reddito sembreranno avere valori altissimi,
#   altre con reddito alto sembreranno avere valori bassi.
# Quindi, al grafico a dispersione, il pattern ‚Äúlineare‚Äù quasi sparisce ‚Üí la correlazione scende.

# Conclusione:
# - Media di income2quasi invariata.
# - Deviazione standard molto pi√π grande.
# - Correlazione con Balance diminuisce, perch√© il rumore rende Income meno informativo.

# B. Costruiamo e analizziamo un modelllo di regressione OLS multi-variata

# pacchetti
library(ISLR)
library(caret)   # per la divisione del dataset
library(MASS)    # per lm se serve

# carico dataset
data(Credit)

# 0. suddivisione tra training e test (70% training, 30% test)
set.seed(123)  # per riproducibilit√†
train_index <- createDataPartition(Credit$Balance, p = 0.7, list = FALSE)
train_data <- Credit[train_index, ]
test_data  <- Credit[-train_index, ]

# 1. Modello di regressione: Balance ~ Income

fit <- lm(Balance ~ ., data = train_data)

# 2. Sommario con coefficienti e p-value
summary_fit <- summary(fit)
print(summary_fit)

# RSE = stima della dev std del termine d‚Äôerrore, espresso nella scala della risposta

# 3. Coefficienti con p-value
coeff_table <- summary_fit$coefficients
print(coeff_table)

# 4. Intervalli di confidenza al 95%
conf_intervals <- confint(fit, level = 0.95)
print(conf_intervals)

# 5. Analisi dei residui (solo per regr. MV)
par(mfrow = c(2,2))   # layout 2x2
plot(fit)

# Interpretazione
# - Coefficienti (inclusa l‚Äôintercetta): si moltiplicano per k ‚Üí qui, raddoppiano.
# - Errori standard: si moltiplicano per k.
# - t-stat e p-value: immutati (perch√© Œ≤ e SE scalano entrambi di k).
# - R¬≤ e F-stat: immutati.
# - Residual standard error (sigma): si moltiplica per k.
# - Residui: scalano di k.
# - In breve: scalare solo la risposta cambia la scala di tutto (Œ≤, SE, residui), ma non l‚Äôevidenza statistica (t, p, R¬≤, F).


# Interpretazione dei risultati (di chatGPT)
# üìå Risultati tipici della regressione
# Il modello stima Balance (saldo residuo della carta di credito) in funzione di tutte le altre variabili socio-demografiche e finanziarie del dataset.
# R¬≤: di solito > 0.7 ‚Üí il modello spiega bene la variabilit√† di Balance.
# üîë Variabili pi√π significative (p-value molto bassi)
# Income ‚Üí reddito: coefficiente positivo ‚Üí all‚Äôaumentare del reddito cresce anche il saldo residuo della carta.
# Limit ‚Üí limite di credito: molto significativo e positivo ‚Üí chi ha un limite pi√π alto tende ad avere un saldo pi√π alto.
# Rating ‚Üí punteggio di credito: anch‚Äôesso positivo ‚Üí pi√π alto il rating, maggiore il balance (collegato al fatto che rating e limite sono correlati).
# Student (dummy: s√¨/no) ‚Üí molto significativo: gli studenti tendono ad avere un balance diverso (spesso pi√π alto).

# Variabili meno significative:
# - Age ‚Üí spesso non significativo (p-value alto).
# - Education ‚Üí anche qui poco impatto diretto.
# - Gender, Married, Ethnicity ‚Üí in genere non risultano statisticamente significativi per spiegare il balance.

# üìå Analisi dei residui
# Dai grafici diagnostici di plot(fit):
# - Residuals vs Fitted ‚Üí residui abbastanza sparsi, ma con un po‚Äô di eteroschedasticit√† (varianza crescente).
# - Normal Q-Q ‚Üí residui vicini alla normale, ma con qualche deviazione agli estremi.
# - Scale-Location ‚Üí conferma un po‚Äô di eteroschedasticit√†.
# - Residuals vs Leverage ‚Üí alcuni punti influenti (outlier) con alta leva, che andrebbero indagati.

# üìå Conclusione
# - Il modello lineare spiega bene Balance, ma gran parte della forza predittiva √® concentrata su poche variabili: Limit, Rating, Income, Student.
# - Le altre variabili hanno scarso potere esplicativo.
# - I residui mostrano che il modello √® buono ma non perfetto (alcuni outlier e un po‚Äô di eteroschedasticit√†).
# üëâ In pratica: chi ha limiti di credito alti, redditi pi√π elevati e/o √® studente, tende ad avere i saldi pi√π alti sulle carte.

# 6. predizioni sul test
pred <- predict(fit, newdata = test_data)

# 7. calcolo RMSE
rmse <- sqrt(mean((test_data$Balance - pred)^2))
rmse

# 8. Modello di regressione: Balance ~ .

# 9. rifare passi precedenti

# 10.modello senza i predittori non significativi

# 11. modifichiamo il dataset Credit
# - la media di Balance resta uguale ma la devstd raddoppia
# - tutte le altre colonne invariate

# Controlliamo media e devstd attuali
mean(Credit$Balance)
sd(Credit$Balance)

# Vogliamo stessa media, ma devstd raddoppiata
mu <- mean(Credit$Balance)
sigma <- sd(Credit$Balance)
target_sigma <- 2 * sigma

# Trasformazione lineare: (X - mu) * (target_sigma/sigma) + mu
Credit$Balance <- (Credit$Balance - mu) * (target_sigma/sigma) + mu

# Controllo nuova media e devstd
mean(Credit$Balance)   # deve essere ~520
sd(Credit$Balance)     # deve essere ~920

# Rifare Regressione

# standardizzazione

# design matrix
# - model.matrix(lm.fit)
# - model.matrix(formula)
# - aumento spread X-i con chatGPT B--> impatto

# VIF(model)

# statistiche mancanti in R base ------------------------------------------

# MODA (da Universit√† La Sapienza):

# per variabili numeriche.

Mode=function(x,tie.choice=c("all","first","random"),message=F) {

  if(!(is.vector(x)|is.factor(x))) stop("x is neither a vector nor a factor!!")

  ux=unique(x)
  tab=tabulate(match(x, ux))
  out.complete=ux[which(tab==max(tab))]
  out = switch(tie.choice[1],
               all = out.complete,
               first = sort(out.complete)[1],
               random = sample(out.complete)[1]
  )
  if(message==T){
    attr(out,"uniqueness")="yes"
    if(length(out.complete)>1){
      attr(out,"uniqueness")="no"
    }
  }
  return(out)
}

# SKEWNESS e KURTOSIS (da RIA, 2nd ed. pp. 139-140):

mystats <- function(x,na.omit=FALSE) {
  if(na.omit)
    x <- x[!is.na(x)]
  m <- mean(x)
  n <- length(x)
  s <- sd(x)
  skew <- sum((x-m)^3/s^3)/n
  kurt <- sum((x-m)^4/s^4)/n - 3
  return(c(n=n,mean=m,sd=s,skew=skew,kurtosis=kurt))
}

# Missing Values (MV) management --------------------------------------------------

# --> in R chiamati NA (Not Available)

data(sleep,package="VIM") # VIM √® un package per la gestione dei MV
data(package="VIM")       # la lista dei dataset SOLO del package indicato (VIM)
# carica in memoria uno specifico dataset
# <Promise object> ? (vedi pane Environment): click sopra.
dim(sleep)
View(sleep)

# MV detection (so: 24027605):
is.na(sleep$Sleep)                       # vettore di booleani (la colonna 'sleep' del df 'sleep'), scomodo da leggere se grandi dimensioni;
# funzionamenmto vettoriale di R: 1 comando applicato a tutti gli elementi del vettore;
# --> True significa NA.

sum(is.na(sleep$Sleep))                  # per singola colonna (ma sono tante!)
sum(is.na(sleep))                        # in totale sull'intero dataframe (ma in quali righe?)
apply(sleep,2,function(x) sum(is.na(x))) # sinossi utile (per colonna)
apply(sleep,1,function(x) sum(is.na(x))) # sinossi utile (per riga)

sapply(sleep, function(x) sum(is.na(x))) # sinossi utile (lo stesso, con la funzione sapply che restituisce un vettore)

complete.cases(sleep)                     # un vettore di booleani che indicano, per ogni riga, se essa NON ha MV (cio√® √® completa)
# oppure no;
# come suggerisce il nome della funzione ('complete.cases'), TRUE qui significa assenza di NA
sum(complete.cases(sleep))                # the number of rows that do NOT have ANY missing values (ie, complete): 42.
# cio√®, per complemento a 62 (il numero totale di righe), ci sono 20 righe con MV.
sum(!complete.cases(sleep))               # il complemento (l'operatore '!' inverte il booleano)

# ricordandoci che un df pu√≤ essere subsettato (per riga) anche con un vettore di booleani (ogni booleano dice ad R se "quella"
# determinata riga dev'essere estratta oppure no.
sleep[complete.cases(sleep),]             # la lista delle righe complete.
sleep[complete.cases(sleep),][,c("NonD")] # subsetting a cascata: i valori della colonna 'NonD' nelle righe complete.
sleep[c(3,4),]                            # due righe non complete: infatti hanno NA
sleep[c(19,20,21),]                       # idem

sum(!complete.cases(sleep))               # the number of rows that DO have missing values (at least 1): 20
sleep[!complete.cases(sleep),]            # la lista delle righe non complete: ognuna ha almeno un NA

library(mice)
md.pattern(sleep,plot=TRUE) # molte combinazioni
# prima colonna: numero righe con quel pattern; ultima colonna: numero variabili con MV (in quel pattern).
# ogni riga rappresenta un possibile pattern di 'missingness': 0 significa MV per quella colonna, 1 significa non-MV.
# La prima riga descrive il pattern "no MV" (tutti gli elementi sono 1).
# La seconda riga descrive il pattern "no MV eccetto che per le colonne 'Dream' e 'NonD'.
# La prima colonna indica il numero di righe in ogni pattern, l'ultima colonna indica il numero di variabili con MV.
# Riga totale: numero di MV per ogni colonna.

library(VIM)                 # VIM √® un package per la gestione dei MV
aggr(sleep,prop=F,numbers=T,bars=F,cex.axis=0.6) # grafico
# il secondo plot √® la versione grafica della funzione 'md.pattern'

# molte funzioni R hanno l'argomento 'na.rm=T/F';
# usiamo una colonna di 'sleep' che abbia MV, ad es. 'NonD':
sum(sleep$NonD)            # --> la funzione 'sum' √® NA (Not Applicable) perch√® ci sono dei MV
sum(sleep$NonD,na.rm=T)    # ok! (la somma sui 62-14 valori di 'NonD' presenti)

# omit NAs sul dataset PIENO
sleep = na.omit(sleep)
View(sleep) # ??
dim(sleep)

# e nei nostri dataset?
md.pattern(Credit,plot=FALSE)   # no MV
md.pattern(Auto,plot=FALSE)     # no MV
md.pattern(Carseats,plot=FALSE) # no MV
md.pattern(Adv,plot=FALSE)      # no MV
md.pattern(Boston,plot=FALSE)   # no MV

# mappa visiva dei NA, ora:
library(Amelia)
data(sleep,package="VIM") # VIM √® un package per la gestione dei MV
dim(sleep)
missmap(sleep)                  # dim(sleep) --> 62 x 10 --> 620 scalari (singole celle).
# 38 MV in totale (da output di 'md.pattern') --> 6%
missmap(Credit)

# altro su MV/NA √® nel corso R avanzato.

